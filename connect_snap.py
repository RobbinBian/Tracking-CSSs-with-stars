"""
This script processes TNG50 simulation snapshots to perform star particle matching for galaxies
with stellar mass greater than 1e7. The intermediate file `starID_1e7_gals_snap_{snap}.hdf5`,
generated by `StarID_repository.py`, stores star particle data for galaxies above this mass threshold
for each snapshot. 

Purpose:
- For a given snapshot, the script identifies specific galaxies by their IDs (e.g., at `snapnum=99`)
  and finds matches for these galaxiesâ€™ star particles in galaxies from other snapshots.
- For each match, it records the galaxy ID (`Galaxy_ID`) in which these particles are found and the 
  number of shared star particles (`Number_of_Stars`).
  
Output:
- The matching results for each snapshot are saved in a CSV file named `MRI_output_{snap}.csv`.
- Each CSV file lists the original galaxy ID, snapshot number, matched galaxy ID, and the count of shared 
  star particles.

Requirements:
- Ensure paths to `starID_1e7_gals_snap_{snap}.hdf5` and relevant data files are correctly set.
"""

import pandas as pd
import numpy as np
import h5py
import csv
import os
from multiprocessing import Pool
from tqdm import tqdm
import illustris_python as il

# Define data paths (local data file paths and output path)
basePath = '/media/bianyuan/data-TNG-1/TNG50-1/output'
desktop_path = "/home/bianyuan/workspace/data/connect_snap/"

def process_snap(snap):
    """
    Process each snapshot to find specific galaxy stars and record them in a CSV file.
    
    Parameters:
    snap (int): Snapshot number
    
    Process:
    - Load the star IDs of each galaxy
    - Load the star IDs of a specific subhalo from TNG data
    - Find common star IDs and record galaxy information
    """
    # Define the output file path
    full_path = os.path.join(desktop_path, f'MRI_output_{snap}.csv')
    
    # Ensure the output directory exists
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    
    # Initialize the CSV file and write the header row
    with open(full_path, 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['ID', 'Snap', 'Galaxy_ID', 'Number_of_Stars'])

    # Open the hdf5 file and process galaxy data
    with h5py.File(f'/media/bianyuan/data-TNG-3/StarID/starID_1e7_gals_snap_{snap}.hdf5', 'r') as f:
        for ids in IDs:
            # Load the star IDs of a specific subhalo
            star_ids = il.snapshot.loadSubhalo(basePath, 99, ids, 'star', fields='ParticleIDs')
            
            # Check each galaxy and find common stars
            for galaxy_id, group in f.items():
                galaxy_star_ids = group['StarID'][:]
                common_stars = set(star_ids).intersection(galaxy_star_ids)

                # If there are common stars, write to the CSV file
                if common_stars:
                    with open(full_path, 'a', newline='') as file:
                        writer = csv.writer(file)
                        row = [ids, snap, galaxy_id, len(common_stars)]
                        writer.writerow(row)
                        print(f"Found {len(common_stars)} matching stars in galaxy ID {galaxy_id} for CSSs3 ID {ids} in snap {snap}.")

# Load specified galaxy IDs and filter data
CSSs3 = pd.read_csv('/home/bianyuan/workspace/data/CSSs3.csv')
IDs = np.array(CSSs3['ID'])[
    np.where(
        ((13.8 - CSSs3['MWted-Age']) >= 2.9) &
        ((13.8 - CSSs3['MWted-Age']) <= 6.4) &
        (np.array(CSSs3['[Z/H]_r']) > 0.1) &
        (CSSs3['Number'] > 1) &
        (CSSs3['Distance_grpvir'] > 0)
    )
]

# Define the list of snapshots to process
specified_snaps = list(range(40, 99))  # Snapshots from 40 to 98

# Main function with multiprocessing to accelerate processing
if __name__ == "__main__":
    # Limit pool size to reduce memory usage
    with Pool(processes=8) as pool:
        # Process the specified snapshots
        pool.map(process_snap, specified_snaps)